{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "cyXEo9TSwLIA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/shotokan/spam-classifier/refs/heads/main/data/sms.tsv\", sep=\"\\t\", names=[\"label\", \"message\"])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ4SsCfgwPIk",
        "outputId": "6a35b9b0-00f6-4544-dcb7-be9f6132618f"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     label                                            message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...    ...                                                ...\n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
            "5568   ham               Will ü b going to esplanade fr home?\n",
            "5569   ham  Pity, * was in mood for that. So...any other s...\n",
            "5570   ham  The guy did some bitching but I acted like i'd...\n",
            "5571   ham                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar segundo dataset\n",
        "temporal_df = pd.read_csv(\"https://raw.githubusercontent.com/shotokan/spam-classifier/refs/heads/main/data/ham-spam.csv\")\n",
        "second_df = temporal_df.rename(columns={\n",
        "    \"IsSpam\": \"label\",\n",
        "    \"Text\": \"message\"\n",
        "})\n",
        "second_df[\"label\"] = second_df[\"label\"].map({0: \"ham\", 1: \"spam\"})\n",
        "print(second_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75-ctB8cNd8A",
        "outputId": "498adff5-d977-4521-e8ea-ca83bb384ecb"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    label                                            message\n",
            "0     ham  key issues going forwarda year end reviews rep...\n",
            "1     ham  congrats contratulations the execution the cen...\n",
            "2     ham   key issues going forwardall under control set...\n",
            "3     ham  epmi files protest entergy transcoattached our...\n",
            "4     ham  california power please contact kristin walsh ...\n",
            "..    ...                                                ...\n",
            "995  spam  somebody nort offlce pro offlce ado phot shop ...\n",
            "996  spam   utf present day course utf reduce mass this p...\n",
            "997  spam   sell regalis for affordable pricehi regalis a...\n",
            "998  spam  email exclusive complimentary satellite dish w...\n",
            "999  spam  unfeigned alilum ciall ambiien aagrra xaanax c...\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mezclar los dataset para teber un dataset con mas datos\n",
        "merged_df = pd.concat([df, second_df], ignore_index=True)\n",
        "print(merged_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8FaCO52NelJ",
        "outputId": "9866d808-f0db-4870-a0c7-d259b678967a"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     label                                            message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...    ...                                                ...\n",
            "6567  spam  somebody nort offlce pro offlce ado phot shop ...\n",
            "6568  spam   utf present day course utf reduce mass this p...\n",
            "6569  spam   sell regalis for affordable pricehi regalis a...\n",
            "6570  spam  email exclusive complimentary satellite dish w...\n",
            "6571  spam  unfeigned alilum ciall ambiien aagrra xaanax c...\n",
            "\n",
            "[6572 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label como número\n",
        "merged_df['label_num'] = merged_df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Feature: longitud del mensaje\n",
        "merged_df['message_length'] = merged_df['message'].apply(len)\n",
        "\n",
        "# Feature: cantidad de signos de puntuación\n",
        "merged_df['punctuation_count'] = merged_df['message'].apply(lambda x: sum(1 for c in x if c in \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"))\n",
        "\n",
        "# Feature: proporción de mayúsculas\n",
        "merged_df['uppercase_ratio'] = merged_df['message'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
        "\n",
        "# Feature: densidad de puntuación\n",
        "merged_df['punctuation_density'] = merged_df['punctuation_count'] / merged_df['message_length']\n",
        "\n",
        "# Feature: contiene URL\n",
        "merged_df['contains_url'] = merged_df['message'].str.contains(r'(?:http[s]?://|www\\.)\\S+', flags=re.IGNORECASE)"
      ],
      "metadata": {
        "id": "v1c-UVsNNhy1"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text = merged_df['message']\n",
        "y = merged_df['label_num']\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "zCvXGOAfNtbB"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF con unigrama + bigrama\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
        "X_test_vec = vectorizer.transform(X_test_text)\n",
        "\n",
        "# Selección de features con chi2\n",
        "selector = SelectKBest(score_func=chi2, k=\"all\")\n",
        "X_train_sel = selector.fit_transform(X_train_vec, y_train)\n",
        "X_test_sel = selector.transform(X_test_vec)"
      ],
      "metadata": {
        "id": "y1C_kfi0OBoY"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chi2: Es una prueba estadística que mide la dependencia entre una variable independiente (una palabra, n-grama, etc.) y la variable objetivo (en este caso, spam o ham). Permite seleccionar solo las características más relevantes y descartar las irrelevantes, mejorando la eficiencia y precisión del modelo.\n",
        "\n",
        "La prueba de Chi-cuadrado (χ²) es una herramienta estadística que mide si hay una dependencia entre dos variables categóricas.\n",
        "\n",
        "En tu proyecto:\n",
        "\n",
        "Una feature puede ser: que el mensaje contenga el trigram \"win cash now\" o la palabra \"http\".\n",
        "\n",
        "La clase es: spam o ham.\n",
        "\n",
        "La prueba de chi2 responde esta pregunta:\n",
        "\n",
        "¿La presencia de esta palabra (o n-grama) está relacionada con que el mensaje sea spam?\n",
        "\n",
        "Si la respuesta es sí, entonces esa palabra es informativa y se considera una buena feature.\n",
        "\n"
      ],
      "metadata": {
        "id": "RC0nXVrZOOkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train_sel, y_train)\n",
        "y_pred = model.predict(X_test_sel)"
      ],
      "metadata": {
        "id": "u7kGo26XOGLj"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred, target_names=['ham', 'spam']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DbF3il4OXp2",
        "outputId": "2758be73-362e-45e2-ea80-87ae90f14a1a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9112576064908722\n",
            "Matriz de confusión:\n",
            " [[1615    0]\n",
            " [ 175  182]]\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.90      1.00      0.95      1615\n",
            "        spam       1.00      0.51      0.68       357\n",
            "\n",
            "    accuracy                           0.91      1972\n",
            "   macro avg       0.95      0.75      0.81      1972\n",
            "weighted avg       0.92      0.91      0.90      1972\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Con tri gramas\n",
        "# Vectorizamos con trigramas únicamente\n",
        "vectorizer_tri = TfidfVectorizer(ngram_range=(1, 3), stop_words='english')\n",
        "X_train_tri = vectorizer_tri.fit_transform(X_train_text)\n",
        "X_test_tri = vectorizer_tri.transform(X_test_text)"
      ],
      "metadata": {
        "id": "ZYoZ66z-OZJf"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selector_tri = SelectKBest(score_func=chi2, k=\"all\")  # puedes ajustar k según necesidad\n",
        "X_train_tri_sel = selector_tri.fit_transform(X_train_tri, y_train)\n",
        "X_test_tri_sel = selector_tri.transform(X_test_tri)"
      ],
      "metadata": {
        "id": "OQAzmeh1Or6e"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_tri = MultinomialNB()\n",
        "model_tri.fit(X_train_tri_sel, y_train)\n",
        "y_pred_tri = model_tri.predict(X_test_tri_sel)"
      ],
      "metadata": {
        "id": "miDu3CoOO10q"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tri))\n",
        "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred_tri, target_names=[\"ham\", \"spam\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gg9kMPaO4qZ",
        "outputId": "0b2ba7e4-2f8b-4d6c-dd68-bcfe104f52be"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9066937119675457\n",
            "Matriz de confusión:\n",
            " [[1615    0]\n",
            " [ 175  182]]\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.90      1.00      0.95      1615\n",
            "        spam       1.00      0.48      0.65       357\n",
            "\n",
            "    accuracy                           0.91      1972\n",
            "   macro avg       0.95      0.74      0.80      1972\n",
            "weighted avg       0.92      0.91      0.89      1972\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo de clasificación basado en texto y utilizando TF-IDF utilizando unigrama y bigrama, junto con selección de características mediante Chi-cuadrado conservando todas las variables (k=\"all\"), logró el mejor rendimiento en las pruebas realizadas y comparando con tri-gramas. Con un accuracy del 91.1% y un F1-score para la clase spam de 0.68, demuestra un excelente balance entre precisión y capacidad de detección. Este resultado confirma que una combinación rica de n-gramas cortos y un filtrado basado en dependencia estadística mejora sustancialmente la capacidad del modelo para detectar mensajes no deseados. Aunque al modelo aún le falta para ser considerado como una base sólida para un sistema de detección de spam real.\n",
        "\n",
        "Necesitamos mas datos para nuestro dataset, así como quizas incluir meta-features que nos puedan ayudar a una mejor clasificación.\n",
        "Del mismo modo, con algún otro modelo como el uso de embeddings podriamos mejorarlo o con LogisticRegression.\n"
      ],
      "metadata": {
        "id": "3qDJPnWBRio8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Entrenar modelo\n",
        "log_model = LogisticRegression(max_iter=1000, class_weight='balanced')  # balanced para mejorar recall en spam\n",
        "log_model.fit(X_train_sel, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred_log = log_model.predict(X_test_sel)\n",
        "\n",
        "# Evaluar\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred_log, target_names=[\"ham\", \"spam\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdA7KYD4S9Df",
        "outputId": "62e4957b-8c28-428e-a09a-1987ba594fbd"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9614604462474645\n",
            "Matriz de confusión:\n",
            " [[1578   37]\n",
            " [  39  318]]\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.98      0.98      1615\n",
            "        spam       0.90      0.89      0.89       357\n",
            "\n",
            "    accuracy                           0.96      1972\n",
            "   macro avg       0.94      0.93      0.93      1972\n",
            "weighted avg       0.96      0.96      0.96      1972\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo basado en regresión logística con penalización por clases (class_weight='balanced'), entrenado sobre vectores TF-IDF seleccionados con Chi², alcanzó un rendimiento sobresaliente:\n",
        "\n",
        "Accuracy: 96.1%\n",
        "\n",
        "Recall para spam: 89%\n",
        "\n",
        "F1-score para spam: 0.89\n",
        "\n",
        "Este modelo supera ampliamente a Naive Bayes, así como también logra una excelente capacidad de detección de mensajes no deseados con un bajo nivel de falsos positivos."
      ],
      "metadata": {
        "id": "RrzlnDu7Uy2i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKSeDxtMU_RD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}